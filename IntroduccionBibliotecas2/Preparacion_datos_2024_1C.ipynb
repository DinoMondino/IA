{"cells":[{"cell_type":"markdown","metadata":{"id":"p6HWJh42Op_a"},"source":["# Prepación de los Datos\n","\n","En la notebook anterior, hicimos el análisis exploratorio de los datos y los dividimos en conjunto de entrenamiento y prueba."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3A1wLs2Op_f"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O_9oYsrySzl8"},"outputs":[],"source":["import sys\n","if 'google.colab' in sys.modules:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    %cd '/content/drive/MyDrive/Inteligencia Artificial/IA - Clases de Práctica/ContenidosPorTemas'\n","    print('google.colab')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jgJkkaNnOp_h"},"outputs":[],"source":["np.random.seed(42)\n","\n","# Cargamos los datos\n","df_housing = pd.read_csv(\"./1_datos/housing.csv\")\n","\n","# Creamos nuestro atributo categórico para los ingresos\n","df_housing[\"income_cat\"] = pd.cut(df_housing[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])\n","\n","# Dividimos los datos en conjunto de entrenamiento y prueba\n","train_set, test_set = train_test_split(df_housing, test_size=0.2, stratify=df_housing[\"income_cat\"], random_state=42)\n","\n","# Eliminamos la categoria income_cat de ambos conjuntos porque no la usamos\n","for set_ in (train_set, test_set):\n","    set_.drop(\"income_cat\", axis=1, inplace=True)"]},{"cell_type":"markdown","metadata":{"id":"0RAlzlddOp_h"},"source":["## Separamos los predictores de las etiquetas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ER-It3jzOp_i"},"outputs":[],"source":["housing = train_set.drop(\"median_house_value\", axis=1)\n","housing_labels = train_set[\"median_house_value\"].copy()"]},{"cell_type":"markdown","metadata":{"id":"GQkI-B3jOp_i"},"source":["## Limpieza de Datos\n","\n","La mayoría de los algoritmos de aprendizaje automático no pueden trabajar con características faltantes, por lo que debemos ocuparnos de esto. Por ejemplo, vimos que el atributo `total_bedrooms` tiene algunos valores faltantes. Existen tres opciones para solucionar esto:\n","\n","1. Eliminar los distritos correspondientes.\n","2. Eliminar el atributo.\n","3. Reemplazar los valores faltantes con algún valor (cero, la media, la mediana, etc.). A esto se llama _imputación_."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cj7NOpOvOp_i"},"outputs":[],"source":["# índices de las filas con datos faltantes\n","null_rows_idx = housing.isnull().any(axis=1)\n","housing.loc[null_rows_idx].head()"]},{"cell_type":"code","source":["housing.loc[null_rows_idx].info()"],"metadata":{"id":"THrtkUJisJbi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h1_oG5S-Op_j"},"outputs":[],"source":["housing_option1 = housing.copy()\n","\n","housing_option1.dropna(subset=[\"total_bedrooms\"], inplace=True)  # opción 1: elimino los distritos con datos faltantes\n","\n","housing_option1.loc[null_rows_idx].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jRxHgk3kOp_k"},"outputs":[],"source":["housing_option2 = housing.copy()\n","\n","housing_option2.drop(\"total_bedrooms\", axis=1, inplace=True)  # opción 2: elimino la columna\n","\n","housing_option2.loc[null_rows_idx].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V7nH05IyOp_k"},"outputs":[],"source":["housing_option3 = housing.copy()\n","\n","median = housing[\"total_bedrooms\"].median()\n","housing_option3[\"total_bedrooms\"].fillna(median, inplace=True)  # opción 3: reemplazo los valores faltantes con la mediana\n","\n","housing_option3.loc[null_rows_idx].head()"]},{"cell_type":"markdown","metadata":{"id":"EkCYx0iYOp_k"},"source":["vamos a optar por la opción 3 ya que es la menos destructiva, pero en lugar del código anterior, usaremos una clase de Scikit-Learn:   `SimpleImputer`.\n","\n","La ventaja que esto posee es que almacenará el valor de la mediana de cada característica. Esto permitirá imputar valores faltantes no solo en el conjunto de entrenamiento, sino también en el conjunto de pruebas y cualquier dato nuevo que ingrese al modelo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C54A-NzbOp_l"},"outputs":[],"source":["from sklearn.impute import SimpleImputer\n","\n","imputer = SimpleImputer(strategy=\"median\")"]},{"cell_type":"markdown","metadata":{"id":"xPPJq5E3Op_m"},"source":["La mediana solo puede calcularse en atributos numéricos, creamos una copia de los datos solo con atributos numéricos (excluyendo `ocean_proximity`)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u6it1dnvOp_m"},"outputs":[],"source":["housing_num = housing.drop(\"ocean_proximity\", axis=1)\n","imputer.fit(housing_num)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BFsXCtebOp_m"},"outputs":[],"source":["imputer.statistics_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dor3N7I1Op_n"},"outputs":[],"source":["housing_num.median(numeric_only=True).values"]},{"cell_type":"markdown","metadata":{"id":"jjEn3hpwOp_n"},"source":["\n","Ahora se puede usar este imputador \"entrenado\" para transformar el conjunto de entrenamiento reemplazando los valores faltantes con las medianas calculadas:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B8xdg1EXOp_n"},"outputs":[],"source":["X = imputer.transform(housing_num) # me devuelve un numpy array\n","type(X)"]},{"cell_type":"markdown","metadata":{"id":"NI15BimkOp_n"},"source":["Los valores faltantes también pueden reemplazarse con el valor medio `(strategy=\"mean\")`, o con el valor más frecuente `(strategy=\"most_frequent\")`, o con un valor constante `(strategy=\"constant\", fill_value=...)`. Las dos últimas estrategias admiten datos no numéricos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pQ6otGPpOp_n"},"outputs":[],"source":["imputer.feature_names_in_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aFdDmFINOp_n"},"outputs":[],"source":["housing_train = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)\n","housing_train.loc[null_rows_idx].head()"]},{"cell_type":"markdown","metadata":{"id":"3rSdowecOp_o"},"source":["## Manejo de atributos categóricos\n","\n","Nuestro conjunto de datos tiene el atributo `ocean_proximity` que es de tipo categórico. La mayoría de algoritmos de aprendizaje maquinal no trabaja con atributos numéricos, por lo que debemos convertir estas categorías de tipo texto a un valor numérico."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MegNLOy5Op_o"},"outputs":[],"source":["housing_cat = housing.loc[:,[\"ocean_proximity\"]]\n","housing_cat.head(8)"]},{"cell_type":"markdown","metadata":{"id":"u663sONaOp_o"},"source":["Primero probaremos la clase `OrdinalEncoder` de Scikit-Learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oigh8gXWOp_o"},"outputs":[],"source":["from sklearn.preprocessing import OrdinalEncoder\n","\n","ordinal_encoder = OrdinalEncoder()\n","housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ws4-P38YOp_o"},"outputs":[],"source":["housing_cat_encoded[:8]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zT2EPSIbOp_o"},"outputs":[],"source":["ordinal_encoder.categories_"]},{"cell_type":"markdown","metadata":{"id":"M8mpjNg1Op_o"},"source":["Un problema con esta representación es que los algoritmos de aprendizaje maquinal asumirán que dos valores cercanos son más similares que dos valores distantes. Esto puede ser aceptable en algunos casos (por ejemplo, para categorías ordenadas como \"malo\", \"regular\", \"bueno\" y \"excelente\"), pero no es el caso para la columna ocean_proximity (por ejemplo, las categorías 0 y 4 son claramente más similares que las categorías 0 y 1).\n","\n","Para solucionar este problema, una solución común es crear un atributo binario por categoría: un atributo igual a 1 cuando la categoría es `<1H OCEAN` (y 0 en caso contrario), otro atributo igual a 1 cuando la categoría es `INLAND` (y 0 en caso contrario), y así sucesivamente. Esto se llama codificación _one-hot_, porque solo un atributo será igual a 1 (activo), mientras que los demás serán 0 (inactivo). Los nuevos atributos a veces se llaman atributos ficticios (atributos _dummy_). Scikit-Learn proporciona la clase OneHotEncoder para convertir valores categóricos en vectores one-hot."]},{"cell_type":"markdown","metadata":{"id":"fs0IVhblOp_o"},"source":["Clase `OneHotEncoder` de Scikit-Learn"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CuopGqlCOp_p"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder\n","\n","cat_encoder = OneHotEncoder()\n","housing_cat_1hot = cat_encoder.fit_transform(housing_cat)\n","housing_cat_1hot"]},{"cell_type":"markdown","metadata":{"id":"B2JW_1bsOp_p"},"source":["Por defecto, la salida de un `OneHotEncoder` es una matriz rala (sparse matrix), en lugar de un arreglo NumPy. Una matriz rala es una representación muy eficiente para matrices que contienen principalmente ceros. Internamente, solo almacena los valores no nulos y sus posiciones. Cuando un atributo categórico tiene cientos o miles de categorías, codificarlo con one-hot resulta en una matriz muy grande llena de 0s excepto por un solo 1 por fila. Una matriz rala ahorra mucha memoria y acelerará los cálculos.\n","\n","Se puede usar una matriz rala de manera similar a un arreglo 2D normal, pero se puede convertir a un arreglo NumPy (denso) con el método `toarray()`. También puede fijarse `sparse=False` al crear el `OneHotEncoder`, en cuyo caso el método `transform()` devolverá directamente un arreglo NumPy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3gRaovthOp_p"},"outputs":[],"source":["housing_cat_1hot.toarray()"]},{"cell_type":"markdown","metadata":{"id":"oQUNDyvfOp_p"},"source":["Cuando entrenamos cualquier estimador de Scikit-Learn utilizando un DataFrame, el estimador guarda los nombres de las columnas en el atributo `feature_names_in_`. Luego, Scikit-Learn asegura que cualquier DataFrame proporcionado a este estimador después de de ser entrenado (por ejemplo, a transform() o predict()) tenga los mismos nombres de columnas. También proporcionan un método `get_feature_names_out()` que se puede usar para construir un DataFrame:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C7UsAw4LOp_p"},"outputs":[],"source":["cat_encoder.feature_names_in_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZVncBe1NOp_p"},"outputs":[],"source":["cat_encoder.get_feature_names_out()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhnUd__9Op_q"},"outputs":[],"source":["df_housing_cat_1hot = pd.DataFrame(housing_cat_1hot.toarray(),\n","                         columns=cat_encoder.get_feature_names_out(),\n","                         index=housing_cat.index)\n","\n","df_housing_cat_1hot"]},{"cell_type":"markdown","metadata":{"id":"P1mVnlC2Op_q"},"source":["## Transformación y escalado de características\n","\n","Una de las transformaciones más importantes es el _escalado de características_ (feature scaling). Con pocas excepciones, los algoritmos de aprendizaje maquinal no resultan en un buen desempeño cuando los atributos numéricos de entrada tienen escalas muy diferentes. En nuestro caso: el número total de habitaciones varía entre 6 a 39,320, los ingresos solo varían desde 0 hasta 15. Sin escalar previamente los datos, el atributo `total_rooms` tendrá una influencia dominante en el modelo de aprendizaje automático ignorando el atributo `median_income`.\n","\n","Existen dos formas: el escalado _min-max_ y la _estandarización_."]},{"cell_type":"markdown","metadata":{"id":"i3JJ7qUcOp_q"},"source":["### Escalado min-max\n","\n","Se lo llama comunmente normalización. Para cada atributo, los valores se escalan en el rango entre 0 a 1. Esto se logra restando el valor mínimo y dividiendo por la diferencia entre el mínimo y el máximo.\n","Scikit-Learn tiene un _transformer_ llamado `MinMaxScaler` que posee un hiperparámetro `feature_range` para cambiar el rango si no se quiere de 0 a 1 (por ejemplo, las redes neuronales funcionan mejor con entradas de media cero, por lo que un rango de -1 a 1 es preferible)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4IYMfoi5Op_q"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","\n","min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n","housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYMObe_6Op_0"},"outputs":[],"source":["housing_num_min_max_scaled"]},{"cell_type":"markdown","metadata":{"id":"rA8xZj2pOp_0"},"source":["### Estandarización\n","\n","Primero se resta el valor medio (los valores estandarizados tienen media de cero), luego divide el resultado por el desvío estándar (los valores tienen un desvío estándar igual a 1). A diferencia del escalado min-max, la estandarización no restringe los valores a un rango específico. La estandarización se ve mucho menos afectada por los valores atípicos.\n","\n","Scikit-Learn proporciona un _transformer_ llamado `StandardScaler`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pWYJL0_OOp_1"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","\n","std_scaler = StandardScaler()\n","housing_num_std_scaled = std_scaler.fit_transform(housing_num)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ztz5tcROp_1"},"outputs":[],"source":["housing_num_std_scaled"]},{"cell_type":"markdown","metadata":{"id":"kn2j7jxaOp_1"},"source":["✅ Si se quiere escalar una matriz rala sin convertirla en una matriz densa, se puede utilizar `StandardScaler` con el hiperparámetro `with_mean` igual a `False`. Esto solo dividirá los datos por el desvío estándar, sin restar la media (sino dejaría de ser rala):"]},{"cell_type":"markdown","metadata":{"id":"3mVz4b2KOp_1"},"source":["Cuando la distribución de una característica está muy sesgada, tanto el escalado min-max como la estandarización comprimirán la mayoría de los valores en un rango pequeño. Por lo tanto, antes de escalar un atributo, se debería transformar para reducir el sesgo y, si es posible, hacer que la distribución sea aproximadamente simétrica (normal).\n","\n","Una forma común de hacer esto para características positivas con sesgo hacia la derecha es reemplazar la característica por su raíz cuadrada (o elevar la característica a una potencia entre 0 y 1).\n","\n","Si la característica tiene una cola muy larga y pesada (sesgo), como una distribución de potencias, se puede reemplazar la característica por su logaritmo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHdiLUlFOp_1"},"outputs":[],"source":["fig, axs = plt.subplots(1, 2, figsize=(15, 7), sharey=True)\n","\n","housing[\"population\"].hist(ax=axs[0], bins=50)\n","housing[\"population\"].apply(np.log).hist(ax=axs[1], bins=50)\n","\n","axs[0].set_xlabel(\"Population\")\n","axs[1].set_xlabel(\"Log of population\")\n","axs[0].set_ylabel(\"Number of districts\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ZnT9Y7DCOp_1"},"source":["Otro enfoque consiste en dividir la característica en \"buckets\" o intervalos. Esto implica dividir su distribución en intervalos de tamaño aproximadamente igual y reemplazar cada valor de la característica con el índice del intervalo al que pertenece. Por ejemplo, podríamos reemplazar cada valor con su percentil. Esto resulta en una característica con una distribución casi uniforme, por lo que no es necesario realizar escalado, o simplemente se puede dividir por el número de intervalos para forzar que los valores estén en el rango de 0 a 1."]},{"cell_type":"code","source":["np.percentile(housing[\"median_income\"], 50)"],"metadata":{"id":"SCNK7rzH5QZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eG0r0ZqHOp_2"},"outputs":[],"source":["percentiles = [ np.percentile(housing[\"median_income\"], p) for p in range(1, 100) ]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9NDS-GrqOp_2"},"outputs":[],"source":["flattened_median_income = pd.cut(housing[\"median_income\"],\n","                                 bins=[-np.inf] + percentiles + [np.inf],\n","                                 labels=range(1, 100 + 1))\n","\n","flattened_median_income.hist(bins=50, figsize=(10,7))\n","plt.xlabel(\"Median income percentile\")\n","plt.ylabel(\"Number of districts\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"4VLdqu70Op_2"},"source":["Hasta ahora, solo hemos analizado las características de entrada, pero es posible que también sea necesario transformar los valores target. Por ejemplo, si la distribución del target tiene un sesgo, podemos aplicarle el logaritmo. Sin embargo, un modelo de regresión predecirá el logaritmo del `median_income`, no el valor en sí. Deberíamos calcular la exponencial de la predicción para obtener el valor buscado."]},{"cell_type":"markdown","metadata":{"id":"rZSYxyHIOp_2"},"source":["## Resumen de la segunda parte: Preparación de los datos\n","\n","En esta segunda parte:\n","- Hicimos una limpieza de los datos de entrenamiento:\n","    - Manejo de datos faltantes o erróneos. Imputamos valores (media, mediana, etc)\n","- Manejo de atributos categóricos\n","    - OrdinalEncoder\n","    - Codificación OneHot\n","- Transformación y escalado de atributos"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}