{"cells":[{"cell_type":"code","source":["import sys\n","\n","from IPython.display import Image, display\n","if 'google.colab' in sys.modules:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","%cd '/content/drive/MyDrive/Inteligencia Artificial/IA - Clases de Práctica/ContenidosPorTemas/ActividadesPracticas/Ensambles'"],"metadata":{"id":"crRAOW6ejwl-"},"id":"crRAOW6ejwl-","execution_count":null,"outputs":[]},{"cell_type":"markdown","id":"416df03c-eded-499e-a5ae-32ce26c34591","metadata":{"id":"416df03c-eded-499e-a5ae-32ce26c34591"},"source":["# Ensambles de Clasificadores\n","\n","La idea clave en los ensambles es lograr **diversidad** en los clasificadores del conjunto. Esta diversidad puede lograrse de distintas maneras.\n","El método más común consiste en usar distintos conjuntos de entrenamiento para entrenar clasificadores individuales. Estos conjuntos se pueden obtener mediante muestreo aleatorio de los datos de entrenamiento.\n","> Cuando el muestreo se realiza con reemplazo, el método de ensamble se denomina **bagging** (boostrap aggregating). Cuando el muestreo se hace sin reemplazo se denomina **pasting**\n","\n","Todos los ensambles deben tener 2 componentes principales: Un algoritmo para generar clasificadores diversos y un método para combinar las salidas de los clasificadores. El método más comun para combinar las salidas es la **votación por mayoría**"]},{"cell_type":"markdown","id":"6d2e055b-41f5-4570-a0dd-48c5ddb1e8a9","metadata":{"id":"6d2e055b-41f5-4570-a0dd-48c5ddb1e8a9"},"source":["## Bagging (Bootstrap aggregation)\n","En esta estrategia, la diversidad del ensamble se obtiene mediante subconjuntos muestreados de forma aleatoria del conjunto de datos original con reemplazo (pueden repetirse los datos). Estos subconjuntos, se utilizan para entrenar diferentes **clasificadores del mismo tipo**. La predicción final, se obtiene a partir de las predicciones individuales utilizando en general **_votación por mayoría_**.\n","El proceso de muestreo de los datos con reemplazo se denomina **Bootstrap**"]},{"cell_type":"code","source":["display(Image(filename='./2_imagenes/bagging.png', width=700))"],"metadata":{"id":"mIm1kjM8lmMv"},"id":"mIm1kjM8lmMv","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"89231c84-a331-4ee2-89c4-811ed2e89533","metadata":{"id":"89231c84-a331-4ee2-89c4-811ed2e89533"},"outputs":[],"source":["from sklearn.datasets import make_moons\n","from sklearn.model_selection import train_test_split\n","from modulos.graficacion import plot_decision_regions\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Creamos datos\n","X, y = make_moons(n_samples=500, noise=0.30, random_state=2)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2, stratify=y)"]},{"cell_type":"code","execution_count":null,"id":"f8bab59e-150c-4604-aa21-d38aa93a08d3","metadata":{"id":"f8bab59e-150c-4604-aa21-d38aa93a08d3"},"outputs":[],"source":["# Visualizamos los datos\n","markers = ('o', '^', 'v')\n","colors = ('salmon','skyblue','red', 'gray', 'cyan')\n","plt.figure(figsize=(8,6))\n","for idx, cl in enumerate(np.unique(y)):\n","    plt.scatter(x=X[y == cl, 0],\n","                y=X[y == cl, 1],\n","                alpha=0.7,\n","                c=colors[idx],\n","                marker=markers[idx],\n","                label=cl,\n","                edgecolor='black')"]},{"cell_type":"code","execution_count":null,"id":"17b8e5bf-b74c-48c1-b24c-2bab58a24070","metadata":{"id":"17b8e5bf-b74c-48c1-b24c-2bab58a24070"},"outputs":[],"source":["tree_clf = DecisionTreeClassifier(random_state=2)\n","tree_clf.fit(X_train, y_train)\n","\n","# Graficación\n","plt.figure(figsize=(8,6))\n","plot_decision_regions(X_train, y_train, tree_clf)\n","plt.xlim(-2,2)\n","plt.ylim(-2,2)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"589a8787-368d-45be-8591-4970ecac1ab3","metadata":{"id":"589a8787-368d-45be-8591-4970ecac1ab3"},"outputs":[],"source":["print(tree_clf.score(X_test, y_test))"]},{"cell_type":"markdown","id":"123cd222-c467-4694-8ada-3fe1f23c950c","metadata":{"id":"123cd222-c467-4694-8ada-3fe1f23c950c"},"source":["**Vamos a comparar el entrenamiento de un árbol de decisión con un ensamble de árboles**\n","\n","**Armamos un ensamble combinando 50 árboles de decisión**\n","\n","Creamos el ensamble usando la clase **_BaggingClassifier_** implementado en el módulo scikit-Learn. Este módulo nos permite seleccionar la técnica de ensamble entre bagging o pasting en función de cómo se seleccionan los parámetros:\n","\n","1. Cuando los subconjuntos se seleccionan de forma aleatoria particionando el dataset sin reemplazo, el algoritmo se conoce como **_Pasting_**\n","2. Cuando los subconjuntos se seleccionan de forma aleatoria particionando el dataset **_con_** reemplazo, el algoritmo se conoce como **_Bagging_**\n","3. Cuando los subconjuntos tienen diferentes características del dataset, el algoritmo se conoce como **_Random Subspaces_**\n","4. Cuando los subconjuntos combinan los dos anteriores, el algoritmo se conoce como **_Random Patches_**"]},{"cell_type":"code","execution_count":null,"id":"563d7a87-ef84-44b8-ad91-694f50cdb3c4","metadata":{"id":"563d7a87-ef84-44b8-ad91-694f50cdb3c4"},"outputs":[],"source":["from sklearn.ensemble import BaggingClassifier\n","\n","bag_clf = BaggingClassifier(DecisionTreeClassifier(),\n","                            n_estimators=50,\n","                            max_samples=100,\n","                            max_features = 1.0,\n","                            bootstrap = True,\n","                            n_jobs=-1,\n","                            random_state=2)\n","bag_clf.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"id":"b61144f3-986b-4113-907a-c20398c48e21","metadata":{"id":"b61144f3-986b-4113-907a-c20398c48e21"},"outputs":[],"source":["# plt.figure(figsize=())\n","plot_decision_regions(X_train, y_train, bag_clf)\n","plt.xlim(-2,2)\n","plt.ylim(-2,2)\n","plt.title(\"Bagging\")\n","plt.ylabel(\"\")\n","\n","plt.show()"]},{"cell_type":"markdown","id":"fde9583c-3186-409d-b493-c1770ff5fac3","metadata":{"id":"fde9583c-3186-409d-b493-c1770ff5fac3"},"source":["### Evaluación Out of bag (OOB)\n","\n","Es una medida del desempeño aplicada a modelos que utilizan la técnica de **_Bootstrapping_**. En la selección de los datos para cada clasificador, durante su entrenamiento, alrededor del 36% de los datos no son muestreados. El desempeño OOB, representa el promedio de los desempeños de cada clasificador del ensamble cuando se tienen en cuenta estos datos que cada clasificador no ve durante el entrenamiento. Esta estrategia puede entenderse como una especie de validación cruzada y es efectiva para estimar la generalización del modelo."]},{"cell_type":"code","execution_count":null,"id":"19c62232-9897-4465-ad9b-701f9444ae72","metadata":{"id":"19c62232-9897-4465-ad9b-701f9444ae72"},"outputs":[],"source":["bag_clf = BaggingClassifier(DecisionTreeClassifier(),\n","                            n_estimators=50,\n","                            max_samples=100,\n","                            max_features = 1.0,\n","                            bootstrap = True,\n","                            oob_score=True,\n","                            n_jobs=-1,\n","                            random_state=2)\n","bag_clf.fit(X_train, y_train)\n","bag_clf.oob_score_"]},{"cell_type":"code","execution_count":null,"id":"07cdd8e5-9e24-4213-9842-501288495c33","metadata":{"id":"07cdd8e5-9e24-4213-9842-501288495c33"},"outputs":[],"source":["print(bag_clf.score(X_test, y_test))"]},{"cell_type":"markdown","id":"ae76efcf-508f-420d-a63a-a3dc5fad38a6","metadata":{"id":"ae76efcf-508f-420d-a63a-a3dc5fad38a6"},"source":["## Random Forest\n","\n","En esta estrategia, los clasificadores son árboles de decisión y los subconjuntos de entrenamiento se eligen por **_Bootstrapping_** o **_Random Subspacing_**.\n","El número de árboles suele ser grande, generalmente entre 100 a 1000 árboles de decisión.\n","\n","**Random Subspacing** genera subconjuntos de entrenamiento con diferentes características de forma aleatoria.\n","\n","Mediante el bootstrapping y random subspacing se introduce aletoriedad al algoritmo, haciendo que cada árbol se forme de manera ligeramente distinta ya que en la formación de los mismos cada uno parte de una muestra diferente y en cada nodo la selección de características cambia."]},{"cell_type":"markdown","id":"69d67b13-b014-47f4-9955-f9d6f8359849","metadata":{"id":"69d67b13-b014-47f4-9955-f9d6f8359849"},"source":["### Parámetros principales del modelo:\n","\n","Este modelo depende principalmente de 2 parámetros:\n","\n","- Número de árboles de decisión. En la función de scikit-learn por defecto es 100.\n","- Número de características (q) que se seleccionan en cada nodo y que permanece fijo en la formación del árbol. En la función de scikit-learn, toma por defecto $q=\\sqrt{F}$ siendo F el número de características totales en el dataset.\n","\n","Una Ventaja de Random Forest es que podemos estimar el desempeño sin necesidad de realizar validación cruzada, usando las muestras OOB."]},{"cell_type":"code","execution_count":null,"id":"e27b744e-59d9-4b91-ba30-6bd663085075","metadata":{"id":"e27b744e-59d9-4b91-ba30-6bd663085075"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","rnd_clf = RandomForestClassifier(n_estimators=50,\n","                                 max_samples=100,\n","                                 n_jobs=-1,\n","                                 max_features=None,\n","                                 random_state=2)\n","rnd_clf.fit(X_train, y_train)"]},{"cell_type":"code","source":["plot_decision_regions(X_train, y_train, rnd_clf)\n","plt.xlim(-2,2)\n","plt.ylim(-2,2)\n","plt.title(\"Random Forest\")\n","plt.ylabel(\"\")\n","\n","plt.show()"],"metadata":{"id":"ffukuvDztY97"},"id":"ffukuvDztY97","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"eee38d2a-0627-41c7-89ba-053c87067023","metadata":{"id":"eee38d2a-0627-41c7-89ba-053c87067023"},"outputs":[],"source":["print(rnd_clf.score(X_test, y_test))"]},{"cell_type":"markdown","id":"301aa77b-d346-46b9-a029-1a858e5e2a8c","metadata":{"id":"301aa77b-d346-46b9-a029-1a858e5e2a8c"},"source":["## Boosting:\n","\n","Boosting se refiere a la forma en como un ensamble puede potenciar (**boost**) un clasificador 'débil' para obtener uno más robusto. Un clasificador 'débil' es aquel que puede clasificar al menos un poco mejor que por decisión aleatoria (para un problema de 2 clases un error de clasificación >= 0.5).\n","\n","En la estrategia de Bagging, los clasificadores se entrenan de forma independiente. En Boosting los clasificadores se entrenan en serie (consecutivamente) y la performance del clasificador _k_ influencia en el entrenamiento del _k+1_.\n","\n","Como idea clave, el algoritmo se enfoca en las muestras mal clasificadas, de forma tal que estas sean muestreadas (**up-sampled**) cuando el siguiente clasificador es entrenado. Por lo tanto, los datos de entrenamiento de los clasificadores siguientes están orientados a instancias cada vez más difíciles de clasificar."]},{"cell_type":"markdown","id":"5408ac4c-50ad-4e20-b9b3-3cdc946b630c","metadata":{"id":"5408ac4c-50ad-4e20-b9b3-3cdc946b630c"},"source":["### Adaboost\n","Es el primer algoritmo que se volvió popular en 1997, creado por Freund y Schapire.\n","\n","Los pasos del algoritmo se pueden describir de la siguiente manera:\n","\n","- Se asigna inicialmente una distribución uniforme de pesos a todas las muestras del conjunto de entrenamiento.\n","\n","    - Cada instancia del conjunto de entrenamiento tiene un peso $\\frac{1}{m}$, con $m$ igual a la cantidad de instancias.\n","\n","- Se selecciona un subconjunto de datos de entrenamiento (con reemplazo) de esta distribución para entrenar el primer clasificador y se obtiene la tasa de error de entrenamiento de dicho clasificador $r_1$:\n","\n","$$r_j=\\frac{\\sum\\limits_{\\begin{matrix} i=1\\\\ y_{p}\\neq y \\end{matrix}}^{m}w^{(i)}}{\\sum\\limits_{i=1}^{m}w^{(i)}}$$\n","\n","- Se le asigna un peso a cada clasificador $\\alpha_j$, $\\eta$ es un coeficiente de aprendizaje del algoritmo:\n","\n","$$\\alpha_j = \\eta \\log\\frac{1-r_j}{r_j}$$\n","\n","- Con este coeficiente se actualiza la distribución de los pesos asignados a las muestras usando el factor calculado $\\alpha_j$. Esta actualización hace que las instancias mal clasificadas por el clasificador anterior se incluyan en los datos de entrenamiento del siguiente clasificador con mayor probabilidad. Es decir, se incrementan los pesos de las muestras mal clasificadas y se disminuyen los de las clasificadas correctamente.\n","\n","\n","$$w^{(i)} = w^{(i)} exp(\\alpha_j), \\qquad y_p^{(i)} \\neq y^{(i)}$$"]},{"cell_type":"markdown","id":"0263f3a2-1b80-420e-b1e2-211f1d00f5e9","metadata":{"id":"0263f3a2-1b80-420e-b1e2-211f1d00f5e9"},"source":["Adaboost usa un esquema de votación no democrático, denominado _weighted majority voting_: aquellos clasificadores que mostraron buena performance durante su entrenamiento son \"recompensados\" con pesos de votación más altos que los demás."]},{"cell_type":"code","source":["display(Image(filename='./2_imagenes/bootstrap.png', width=700))"],"metadata":{"id":"oUK2bn-HdAzt"},"id":"oUK2bn-HdAzt","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"215954cc-83b7-42ba-a69a-dfe92dec33b5","metadata":{"id":"215954cc-83b7-42ba-a69a-dfe92dec33b5"},"outputs":[],"source":["from sklearn.svm import SVC\n","\n","X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","\n","m = len(X_train)\n","\n","plt.figure(figsize=(8,6))\n","learning_rate = 1\n","\n","pesos_instancias = np.ones(m) / m\n","\n","for i in range(1):\n","    print(f\"Iteración {i+1}:\")\n","    svm_clf = SVC(C=100, gamma=0.15)\n","\n","    pesos_instancias = pesos_instancias * m\n","    print(\"pesos de instancias:\", pesos_instancias[:10])\n","\n","    svm_clf.fit(X_train, y_train, sample_weight=pesos_instancias)\n","    y_pred = svm_clf.predict(X_train)\n","\n","    pesos_mal_clasificados = pesos_instancias[y_pred != y_train].sum()\n","    print(\"pesos de los mal clasificados:\", pesos_instancias[y_pred != y_train][:5])\n","    print(\"pesos de los bien clasificados:\", pesos_instancias[y_pred == y_train][:5])\n","    r = pesos_mal_clasificados / pesos_instancias.sum()\n","    print(\"tasa de error de entrenamiento r:\", r)\n","\n","    alpha = learning_rate * np.log((1 - r) / r)\n","    print(\"peso del clasificador:\", alpha)\n","\n","    #Actualización de los pesos\n","    pesos_instancias[y_pred != y_train] *= np.exp(alpha)\n","    #Se normalizan los pesos\n","    pesos_instancias /= pesos_instancias.sum()\n","    print(\"pesos de los mal clasificados:\", pesos_instancias[y_pred != y_train][:5])\n","    print(\"pesos de los bien clasificados:\", pesos_instancias[y_pred == y_train][:5])\n","\n","    plot_decision_regions(X_train, y_train, svm_clf)\n","\n","    plt.title(f\"learning_rate = {learning_rate}\")\n","    plt.text(-0.75, -1.25, \"1\", fontsize=16)\n","    # plt.text(-0.45, -1.25, \"2\", fontsize=16)\n","    # plt.text(0.5, -1.25, \"3\", fontsize=16)\n","    # plt.text(2, 1, \"4\", fontsize=16)\n","    # plt.text(2,  0.75, \"5\", fontsize=16)\n","plt.xlim(-1.5,2.5)\n","plt.ylim(-1.5,2)\n","plt.show()"]},{"cell_type":"markdown","id":"2e48ec77-5a25-45fb-be14-c462e705978e","metadata":{"id":"2e48ec77-5a25-45fb-be14-c462e705978e"},"source":["sklearn utiliza una versión de AdaBoost llamada SAMME _(Stagewise Additive Modeling using a Multiclass Exponential loss function)_ Si los clasificadores en el ensamble pueden realizar predicciones mediante probabilidades, sklearn utiliza la variante SAMME.R que tiene en cuenta las probabilidades en lugar de las predicciones y generalmente obtiene mejores desempeños."]},{"cell_type":"code","execution_count":null,"id":"267b4bab-a1b3-4b67-a071-43dfb4a896a6","metadata":{"id":"267b4bab-a1b3-4b67-a071-43dfb4a896a6"},"outputs":[],"source":["from sklearn.ensemble import AdaBoostClassifier\n","\n","X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n","\n","svc = SVC(\n","    C=100, probability=True, tol=1e-5, gamma=.9\n",")\n","\n","ada_clf = AdaBoostClassifier( svc,\n","                              n_estimators=3,\n","                              learning_rate=0.9,\n","                              random_state=0\n","                             )\n","\n","ada_clf.fit(X_train, y_train)\n","\n","svc.fit(X_train, y_train)\n","\n","plt.figure(figsize=(8,6))\n","plot_decision_regions(X_train, y_train, ada_clf)\n","plt.xlim(-1.5,2.5)\n","plt.ylim(-1.5,2)\n","plt.show()\n","\n","print(\"SVC score:\")\n","print(svc.score(X_test, y_test))\n","print(\"SVC Adaboost score:\")\n","print(ada_clf.score(X_test, y_test))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[],"gpuType":"T4"}},"nbformat":4,"nbformat_minor":5}