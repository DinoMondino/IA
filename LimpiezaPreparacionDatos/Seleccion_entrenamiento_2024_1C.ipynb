{"cells":[{"cell_type":"markdown","metadata":{"id":"_aSNudiS0kpG"},"source":["# Selección, entrenamiento y evaluación de un modelo"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"EPpXE7Rb0kpK","executionInfo":{"status":"ok","timestamp":1710777771213,"user_tz":180,"elapsed":2734,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.pipeline import make_pipeline\n","from sklearn.compose import make_column_selector"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"O_9oYsrySzl8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710777799129,"user_tz":180,"elapsed":27920,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"}},"outputId":"1d12c130-fa71-4f6a-fa8e-4c91747056a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Inteligencia Artificial/IA - Clases de Práctica/ContenidosPorTemas\n","google.colab\n"]}],"source":["import sys\n","if 'google.colab' in sys.modules:\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","    %cd '/content/drive/MyDrive/Inteligencia Artificial/IA - Clases de Práctica/ContenidosPorTemas'\n","    print('google.colab')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmGq2l8e0kpM"},"outputs":[],"source":["np.random.seed(42)\n","\n","# Cargamos los datos\n","df_housing = pd.read_csv(\"./1_datos/housing.csv\")\n","\n","# Creamos nuestro atributo categórico para los ingresos\n","df_housing[\"income_cat\"] = pd.cut(df_housing[\"median_income\"], bins=[0., 1.5, 3.0, 4.5, 6., np.inf], labels=[1, 2, 3, 4, 5])\n","\n","# Dividimos los datos en conjunto de entrenamiento y prueba\n","strat_train_set, strat_test_set = train_test_split(df_housing, test_size=0.2, stratify=df_housing[\"income_cat\"], random_state=42)\n","\n","# Eliminamos la categoria income_cat de ambos conjuntos porque no la usamos\n","for set_ in (strat_train_set, strat_test_set):\n","    set_.drop(\"income_cat\", axis=1, inplace=True)\n","\n","# Separamos predictores y etiquetas\n","housing = strat_train_set.drop(\"median_house_value\", axis=1)\n","housing_labels = strat_train_set[\"median_house_value\"].copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FGYze_x80kpM"},"outputs":[],"source":["num_pipeline = Pipeline([\n","    (\"impute\", SimpleImputer(strategy=\"median\")),\n","    (\"standardize\", StandardScaler()),\n","])\n","\n","cat_pipeline = make_pipeline(\n","    SimpleImputer(strategy=\"most_frequent\"),\n","    OneHotEncoder(handle_unknown=\"ignore\"))\n","\n","\n","log_pipeline = make_pipeline(\n","    SimpleImputer(strategy=\"median\"),\n","    FunctionTransformer(np.log, feature_names_out=\"one-to-one\"),\n","    StandardScaler())\n","\n","\n","preprocessing = ColumnTransformer(\n","    [\n","        (\"log\", log_pipeline, [\"total_bedrooms\", \"total_rooms\", \"population\",\n","                               \"households\", \"median_income\"]),\n","        (\"cat\", cat_pipeline, make_column_selector(dtype_include=object)),\n","    ],\n","    remainder=num_pipeline\n","\n","    )"]},{"cell_type":"markdown","metadata":{"id":"Stj4J5OV0kpN"},"source":["## Selección y Entrenamiento\n","\n","¡Finalmente! Hemos resuelto el problema, obtuvimos los datos, los exploramos, los dividimos en un conjunto de entrenamiento y un conjunto de prueba, y creamos un _pipeline_ de preprocesamiento para limpiar y preparar automáticamente los datos para los algoritmos de aprendizaje maquinal. Ahora estamos listos para seleccionar y entrenar un modelo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JmP7wzpV0kpN"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","\n","lin_reg = make_pipeline(preprocessing, LinearRegression())\n","lin_reg.fit(housing, housing_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rDO5r8dy0kpO"},"outputs":[],"source":["housing_predictions = lin_reg.predict(housing)\n","housing_predictions[:5].round()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJhBMdWn0kpP"},"outputs":[],"source":["housing_labels.iloc[:5].values"]},{"cell_type":"markdown","metadata":{"id":"2VMjTDFj0kpP"},"source":["Vamos a usar la raiz del error cuadrático medio (RMSE) como medida de desempeño"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXAkHnPs0kpQ"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error\n","\n","lin_rmse = mean_squared_error(housing_labels, housing_predictions, squared=False)\n","lin_rmse"]},{"cell_type":"markdown","metadata":{"id":"sDzWbqpL0kpQ"},"source":["El valor de `median_house_value` de los distritos oscila entre \\$120,000 y \\$265,000, por lo que un error de predicción de \\$71,834 no es muy bueno. Este es un ejemplo de un modelo mal ajustado (`underfitting`) a los datos de entrenamiento. Cuando esto ocurre, puede significar que las características no proporcionan suficiente información para realizar buenas predicciones o que el modelo no es lo suficientemente bueno.\n","\n","Formas de corregir el desajuste son seleccionar un modelo más potente, proporcionar al algoritmo mejores características o reducir las restricciones en el modelo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VwfJ1Qk00kpQ"},"outputs":[],"source":["from sklearn.tree import DecisionTreeRegressor\n","\n","tree_reg = make_pipeline(preprocessing, DecisionTreeRegressor(random_state=42))\n","tree_reg.fit(housing, housing_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jBlIjZje0kpQ"},"outputs":[],"source":["housing_predictions = tree_reg.predict(housing)\n","tree_rmse = mean_squared_error(housing_labels, housing_predictions,squared=False)\n","tree_rmse"]},{"cell_type":"markdown","metadata":{"id":"tnXoT5iV0kpQ"},"source":["¿Ningún error en absoluto? Es probable que el modelo se haya sobreajustado (`overfitting`) a los datos. La situación más probable es que el modelo haya memorizado los datos de entrenamiento en lugar de aprender patrones generales que puedan aplicarse a nuevos datos. El sobreajuste ocurre cuando un modelo se ajusta demasiado a los detalles específicos de los datos de entrenamiento y, como resultado, tiene un rendimiento deficiente en datos nuevos y no vistos.\n","\n","¿Cómo podemos comprobarlo? ¿Usamos los datos de prueba? Como dijimos anteriormente, no miramos el conjunto de prueba ni lo usamos, hasta que un modelo entrenado esté listo para usar, por lo que necesitas utilizar parte del conjunto de entrenamiento para el entrenamiento y otra parte para la validación del modelo."]},{"cell_type":"markdown","metadata":{"id":"P1QHhI9e0kpR"},"source":["## Validación del Modelo"]},{"cell_type":"markdown","metadata":{"id":"TH9JgHB70kpR"},"source":["Usamos validación cruzada"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sy-YUeI60kpR"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score\n","\n","tree_rmses = -cross_val_score(tree_reg, housing, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1g23m2qg0kpR"},"outputs":[],"source":["pd.Series(tree_rmses).describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"49CyB1zq0kpR"},"outputs":[],"source":["lin_rmses = -cross_val_score(lin_reg, housing, housing_labels,\n","                              scoring=\"neg_root_mean_squared_error\", cv=10)\n","pd.Series(lin_rmses).describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"90P5CP4p0kpS"},"outputs":[],"source":["from sklearn.ensemble import RandomForestRegressor\n","\n","forest_reg = make_pipeline(preprocessing, RandomForestRegressor(random_state=42))\n","forest_rmses = -cross_val_score(forest_reg, housing, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=10)\n","pd.Series(forest_rmses).describe()"]},{"cell_type":"markdown","metadata":{"id":"IPIEg02t0kpS"},"source":["## Ajuste de hiperparámetros del modelo\n","\n","Usaremos GridSearch para ajustar los parámetros de nuestros modelos (Lo veremos en las siguientes clases)"]},{"cell_type":"markdown","metadata":{"id":"8zMhuS-LnQ2h"},"source":["## Evaluación con los datos de prueba"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1BHiV4sZ3ax6"},"outputs":[],"source":["final_model = forest_reg\n","\n","X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n","y_test = strat_test_set[\"median_house_value\"].copy()\n","\n","final_predictions = final_model.predict(X_test)\n","\n","final_rmse = mean_squared_error(y_test, final_predictions, squared=False)\n","print(final_rmse)"]}],"metadata":{"accelerator":"TPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}