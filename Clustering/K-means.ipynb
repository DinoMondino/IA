{"cells":[{"cell_type":"markdown","id":"528b04b9-2705-4ac9-bfad-03b265e82e93","metadata":{"id":"528b04b9-2705-4ac9-bfad-03b265e82e93"},"source":["## Agrupamiento o Clustering"]},{"cell_type":"markdown","id":"9f3dbb56-becc-4d46-a88d-b22406b876a0","metadata":{"id":"9f3dbb56-becc-4d46-a88d-b22406b876a0"},"source":["__Clustering__ es una técnica de __aprendizaje no supervisado__ que permite descubrir estructuras ocultas en datos no etiquetados. Esta técnica tiene como objetivo encontrar un agrupamiento natural de los datos donde los items pertenecientes a un mismo grupo son más similares entre si que aquellos pertenecientes a grupos diferentes."]},{"cell_type":"markdown","id":"ab115496-1042-4a64-b390-1aa95a7b31ef","metadata":{"id":"ab115496-1042-4a64-b390-1aa95a7b31ef"},"source":["### K-means\n","Es el algoritmo de clustering más popular, es uno de los más simples de implementar y a su vez eficiente comparado a otros algoritmos.\n","Este algoritmo pertenece a la categoría __clustering basado en prototipos__, es decir cada agrupamiento está representado por un prototipo como el __centroide__ o puntos similiares en el caso de características continuas, o el __medoid__(el punto más representativo que minimiza la distancia a todos los puntos del agrupamiento) en el caso de variables categóricas.\n","\n","Una de las desventajas de este algoritmo, es que debemos especificar el número de clusters _k_ _a priori_"]},{"cell_type":"code","source":["import sys\n","\n","from IPython.display import Image, display\n","if 'google.colab' in sys.modules:\n","    from google.colab import drive\n","    drive.mount('/content/drive')"],"metadata":{"id":"zBMe6i1suHoD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717430357312,"user_tz":180,"elapsed":33481,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"}},"outputId":"2802c477-8c4b-4a8b-e0d1-1f02c29c193b"},"id":"zBMe6i1suHoD","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Inteligencia Artificial/IA - Clases de Práctica/ContenidosPorTemas/ActividadesPracticas/Clustering'"],"metadata":{"id":"q8T1dOkmuM9V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717430358599,"user_tz":180,"elapsed":1301,"user":{"displayName":"Diana Carolina VERTIZ DEL VALLE","userId":"04236681262725283456"}},"outputId":"ffb14a6b-66fd-4cf4-b489-76f71dab1327"},"id":"q8T1dOkmuM9V","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Inteligencia Artificial/IA - Clases de Práctica/ContenidosPorTemas/ActividadesPracticas/Clustering\n"]}]},{"cell_type":"code","execution_count":null,"id":"3bf51c11-9616-4787-8556-2f9fe2e28666","metadata":{"id":"3bf51c11-9616-4787-8556-2f9fe2e28666"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","source":["A continuación vamos a crear un dataset de juguete con la función `make_blobs` para generar datos. `blob_centers` y `blob_std`: Define los centros y desviaciones estándar de los clusters en el espacio 2D."],"metadata":{"id":"9u4eTe5Pqjip"},"id":"9u4eTe5Pqjip"},{"cell_type":"code","execution_count":null,"id":"e1085a9c-fae3-48c5-b786-c98e56a9ca77","metadata":{"id":"e1085a9c-fae3-48c5-b786-c98e56a9ca77"},"outputs":[],"source":["# Creamos un dataset de juguete\n","from sklearn.datasets import make_blobs\n","\n","blob_centers = np.array(\n","    [[ 0.2,  2.3],\n","     [-1.5 ,  2.3],\n","     [-2.8,  1.8],\n","     [-2.8,  2.8],\n","     [-2.8,  1.3]])\n","blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])"]},{"cell_type":"markdown","source":["`X, y = make_blobs(...)`: Genera 2000 puntos de datos distribuidos en blobs (aglomerados) con los centros y desviaciones estándar especificados."],"metadata":{"id":"Q4BctxLqsC1X"},"id":"Q4BctxLqsC1X"},{"cell_type":"code","source":["X, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std, random_state=7)"],"metadata":{"id":"N3onS3nUsCY1"},"id":"N3onS3nUsCY1","execution_count":null,"outputs":[]},{"cell_type":"code","source":["X"],"metadata":{"id":"lIUZAdF1tM4j"},"id":"lIUZAdF1tM4j","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"4a82883c-3804-4262-a7d0-c1b72a89c128","metadata":{"id":"4a82883c-3804-4262-a7d0-c1b72a89c128"},"outputs":[],"source":["def plot_clusters(X, y=None):\n","    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n","    plt.xlabel(\"$x_1$\", fontsize=14)\n","    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"]},{"cell_type":"code","execution_count":null,"id":"f08966be-dcd6-42fc-86fe-82d708d880b9","metadata":{"id":"f08966be-dcd6-42fc-86fe-82d708d880b9"},"outputs":[],"source":["plt.figure(figsize=(8, 4))\n","plot_clusters(X)\n","plt.show()"]},{"cell_type":"markdown","id":"eec1de47-7163-4397-8000-bc05417353a7","metadata":{"id":"eec1de47-7163-4397-8000-bc05417353a7"},"source":["### Entrenamiento y predicción"]},{"cell_type":"code","execution_count":null,"id":"7c46de7f-2752-491a-a3fc-ba67a371124e","metadata":{"id":"7c46de7f-2752-491a-a3fc-ba67a371124e"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","\n","k = 5\n","kmeans = KMeans(n_clusters=k, n_init=10, random_state=1)\n","kmeans.fit(X)"]},{"cell_type":"markdown","id":"d541c5a0-0351-4d20-aa29-3b16450a6bce","metadata":{"id":"d541c5a0-0351-4d20-aa29-3b16450a6bce"},"source":["Los centroides estimados:"]},{"cell_type":"code","execution_count":null,"id":"47510fcd-9e17-4d86-9905-23c8f75ad687","metadata":{"id":"47510fcd-9e17-4d86-9905-23c8f75ad687"},"outputs":[],"source":["kmeans.cluster_centers_"]},{"cell_type":"markdown","id":"258c4bc3-16e3-44cd-af20-5089003029d5","metadata":{"id":"258c4bc3-16e3-44cd-af20-5089003029d5"},"source":["Las etiquetas que predice KMeans corresponde a los índices del cluster al que un dato es asignado."]},{"cell_type":"code","execution_count":null,"id":"368c5880-1f3c-4e8b-9e94-3b7be3b985f9","metadata":{"id":"368c5880-1f3c-4e8b-9e94-3b7be3b985f9"},"outputs":[],"source":["kmeans.labels_"]},{"cell_type":"markdown","id":"9d25fe64-2de2-4eca-bd5a-c32fbbab9daf","metadata":{"id":"9d25fe64-2de2-4eca-bd5a-c32fbbab9daf"},"source":["Podemos predecir las etiquetas de nuevas instancias"]},{"cell_type":"code","execution_count":null,"id":"516f1947-4585-49f5-a731-8c9e76f10557","metadata":{"id":"516f1947-4585-49f5-a731-8c9e76f10557"},"outputs":[],"source":["X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])\n","kmeans.predict(X_new)"]},{"cell_type":"markdown","id":"3cf09940-7812-4592-889a-a54889b544fc","metadata":{"id":"3cf09940-7812-4592-889a-a54889b544fc"},"source":["Si graficamos las regiones de decisión podemos ver un diagrama conocido como _Voronoid tessellation_"]},{"cell_type":"code","execution_count":null,"id":"8705efbe-596b-4dbb-95a3-0120b69527a7","metadata":{"id":"8705efbe-596b-4dbb-95a3-0120b69527a7"},"outputs":[],"source":["from modulos.graficacion import plot_decision_boundaries\n","\n","plt.figure(figsize=(8, 4))\n","plot_decision_boundaries(kmeans, X)\n","plt.show()"]},{"cell_type":"markdown","id":"955aa487-a315-4527-aa08-aba65cbfc01c","metadata":{"id":"955aa487-a315-4527-aa08-aba65cbfc01c"},"source":["En la clase KMeans, el método transform() transforma los puntos en la distancia desde cada instancia a todos los centroides:"]},{"cell_type":"code","execution_count":null,"id":"b97c35b1-a577-4ec3-a5f3-33f7cfc7a967","metadata":{"id":"b97c35b1-a577-4ec3-a5f3-33f7cfc7a967"},"outputs":[],"source":["kmeans.transform(X_new)"]},{"cell_type":"markdown","id":"3b04253a-8f13-4656-b85a-0d06e2971f6c","metadata":{"id":"3b04253a-8f13-4656-b85a-0d06e2971f6c"},"source":["Si tuvieramos un dataset multidimensional (>>k) y lo transformamos de esta manera, nos quedaría un dataset k-dimensional. Esta transformación podría ser una **técnica de reducción dimensional no lineal** muy eficiente."]},{"cell_type":"markdown","id":"3ff2c5f6-a8cc-438d-b557-8a7e17c779c2","metadata":{"id":"3ff2c5f6-a8cc-438d-b557-8a7e17c779c2"},"source":["### Algoritmo k-means\n","El algoritmo de _k-means_ puede resumirse mediante los siguientes pasos:\n","\n","1. Elegir de forma aleatoria _k_ centroides de las muestras como centros iniciales de los agrupamientos.\n","2. Asignar cada muestra al centroide más cercano $\\mu^{(j)}$, _j_ $\\in$ {_1_,...,_k_}.\n","3. Actualizar la ubicación de los centroides, calculando la media de todas las muestras asignadas a cada centroide. Se calcula, la diferencia entre el centroide anterior y el nuevo y se lo compara con la tolerancia definida.\n","4. Repetir los pasos 2 y 3 hasta que la tolerancia definida por el usuario o el número máximo de iteraciones sea alcanzado.\n","\n","La similitud entre la muestras se mide con la __distancia cuadrática Euclídea__. Entonces, en base a la distancia euclídea se puede describir el algoritmo de k-means como un problema de optimización para minimizar la __suma de los errores cuadráticos (SSE)__:\n","\n","$$SSE = \\sum\\limits_{i=1}^n\\sum\\limits_{j=1}^k w^{(i,j)}\\lVert  x^{(i)} - \\mu^{(j)}  \\rVert_2^2$$"]},{"cell_type":"markdown","id":"173509d1-2dda-4d13-b7fb-2f9b4f4ca81c","metadata":{"id":"173509d1-2dda-4d13-b7fb-2f9b4f4ca81c"},"source":["$\\mu^{(j)}$ es el centroide en el agrupamiento _j_. $w^{(i,j)}$ es igual a 1, si la muestra $x^{(i)}$ pertenece al cluster _j_, de lo contrario, será igual a 0. Esta medida __SSE__ también se conoce como __Inercia__ cuando se obtiene la media de las distancias cuadráticas. La inercia puede entenderse como una medida de cuanta cohesión interna poseen los agrupamientos.\n"]},{"cell_type":"code","execution_count":null,"id":"b2463fdc-226e-423c-91d6-1b30657f4a47","metadata":{"id":"b2463fdc-226e-423c-91d6-1b30657f4a47"},"outputs":[],"source":["kmeans_iter1 = KMeans(n_clusters=5, init=\"random\", n_init=1,  max_iter=1, random_state=1)\n","kmeans_iter2 = KMeans(n_clusters=5, init=\"random\", n_init=1,  max_iter=2, random_state=1)\n","kmeans_iter3 = KMeans(n_clusters=5, init=\"random\", n_init=1,  max_iter=3, random_state=1)\n","kmeans_iter4 = KMeans(n_clusters=5, init=\"random\", n_init=1,  max_iter=5, random_state=1)\n","kmeans_iter5 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=1) #max_iter=300\n","kmeans_iter1.fit(X)\n","kmeans_iter2.fit(X)\n","kmeans_iter3.fit(X)\n","kmeans_iter4.fit(X)\n","_=kmeans_iter5.fit(X)"]},{"cell_type":"code","execution_count":null,"id":"ec287a15-ec14-4de0-8cac-0df0851966a7","metadata":{"id":"ec287a15-ec14-4de0-8cac-0df0851966a7"},"outputs":[],"source":["from modulos.graficacion import plot_data, plot_centroids,plot_decision_boundaries\n","\n","plt.figure(figsize=(10, 8))\n","\n","plt.subplot(321)\n","plot_data(X)\n","plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')\n","plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n","plt.tick_params(labelbottom=False)\n","plt.title(\"Actualización de los centroides (inicialmente aleatorio)\", fontsize=14)\n","\n","plt.subplot(322)\n","plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False, show_ylabels=False)\n","plt.title(\"Etiqueta instancias\", fontsize=14)\n","\n","plt.subplot(323)\n","plot_decision_boundaries(kmeans_iter2, X, show_centroids=False, show_xlabels=False)\n","plot_centroids(kmeans_iter2.cluster_centers_)\n","\n","plt.subplot(324)\n","plot_decision_boundaries(kmeans_iter3, X, show_centroids=False, show_xlabels=False, show_ylabels=False)\n","plot_centroids(kmeans_iter3.cluster_centers_)\n","\n","plt.subplot(325)\n","plot_decision_boundaries(kmeans_iter4, X, show_centroids=False)\n","plot_centroids(kmeans_iter4.cluster_centers_)\n","\n","plt.subplot(326)\n","plot_decision_boundaries(kmeans_iter5, X, show_ylabels=False)\n","\n","plt.show()"]},{"cell_type":"markdown","id":"592d4b12-6fe7-4d97-958d-b18bab13bc6f","metadata":{"id":"592d4b12-6fe7-4d97-958d-b18bab13bc6f"},"source":["Aunque el algoritmo de k-means garantiza su convergencia, podría no converger a la solución correcta. Esto dependerá de la inicialización del centroide como se ve a continuación usando distintas semillas:"]},{"cell_type":"code","execution_count":null,"id":"934457af-1282-4c0d-a6e4-2c9227caccb7","metadata":{"id":"934457af-1282-4c0d-a6e4-2c9227caccb7"},"outputs":[],"source":["kmeans_rnd_init1 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=0)\n","kmeans_rnd_init2 = KMeans(n_clusters=5, init=\"random\", n_init=1, random_state=2)\n","kmeans_rnd_init1.fit(X)\n","kmeans_rnd_init2.fit(X)"]},{"cell_type":"code","execution_count":null,"id":"2515fe5e-f156-4f58-b978-abb5a39b562a","metadata":{"id":"2515fe5e-f156-4f58-b978-abb5a39b562a"},"outputs":[],"source":["plt.figure(figsize=(12, 4))\n","\n","plt.subplot(121)\n","plot_decision_boundaries(kmeans_rnd_init1, X)\n","plt.title(\"Solución 1\", fontsize=14)\n","\n","plt.subplot(122)\n","plot_decision_boundaries(kmeans_rnd_init2, X, show_ylabels=False)\n","plt.title(\"Solución 2\", fontsize=14)"]},{"cell_type":"markdown","id":"8b74046b-2d4f-475c-8205-89f1f6b193d0","metadata":{"id":"8b74046b-2d4f-475c-8205-89f1f6b193d0"},"source":["Si supieramos de antemano la ubicación aproximada de los centroides, podríamos utilizar el hiperparámetro _init_ para inicializar los centroides mediante un array de numpy"]},{"cell_type":"code","execution_count":null,"id":"09979125-e436-45be-9001-95427b1654b6","metadata":{"id":"09979125-e436-45be-9001-95427b1654b6"},"outputs":[],"source":["centroides_iniciales = np.array([[-3, 3], [-3, 2], [-3, 1], [-1, 2], [0, 2]])\n","kmeans = KMeans(n_clusters=5, init=centroides_iniciales, n_init=1, random_state=0)\n","kmeans.fit(X)\n","plot_decision_boundaries(kmeans, X, show_xlabels=True, show_ylabels=True)"]},{"cell_type":"markdown","id":"617e88d1-a00e-4399-add1-9fc221651333","metadata":{"id":"617e88d1-a00e-4399-add1-9fc221651333"},"source":["### K-means++\n","\n","El algoritmo clásico K-means, utiliza una semilla aleatoria para posicionar los centroides iniciales, esto puede dar como resultado un mal agrupamiento o una lenta convergencia si los centroides iniciales son mal seleccionados.\n","\n","En la estrategia K-means++, los centroides son inicializados distantes entre sí dando una mayor probabilidad de obtener mejores resultados.\n","\n","Para utilizar este algoritmo con el módulo de scikit-learn, se debe cambiar init='random' por **init='k-means++'** aunque esta opción, es la opción por defecto.\n","\n","Notar que el hiperparámetro init, puede recibir un array con los centroides iniciales o un string con el tipo de algoritmo a utilizar"]},{"cell_type":"markdown","id":"6baae435-8f6f-4cc8-9b82-54597a9dd35e","metadata":{"id":"6baae435-8f6f-4cc8-9b82-54597a9dd35e"},"source":["### Inercia\n","Una solución adicional a la convergencia, es ejecutar muchas veces el algoritmo con inicializaciones aleatorias y quedarse con la mejor solución. Este número de inicializaciones es controlado por el hiperparámetro `n_init` que por defecto es 10. Scikit-Learn implementa esta solución por defecto.\n","\n","La metrica que utiliza para seleccionar la mejor solución es la __Inercia__ del modelo. La inercia es la distancia cuadrática media entre cada instancia y su centroide más cercano.\n","Se puede obtener la inercia del modelo haciendo:"]},{"cell_type":"code","execution_count":null,"id":"a39384b2-b9d9-4df1-9391-ccd7947e0858","metadata":{"id":"a39384b2-b9d9-4df1-9391-ccd7947e0858"},"outputs":[],"source":["kmeans.inertia_"]},{"cell_type":"markdown","id":"8a7b0376-97ad-4823-8ce6-a80f98e25a8e","metadata":{"id":"8a7b0376-97ad-4823-8ce6-a80f98e25a8e"},"source":["La inercia tiene sus desventajas:\n","- supone que los agrupamientos son convexos e isotrópicos. No responde adecuadamente a agrupamientos elongados o con formas irregulares.\n","- La inercia no es una métrica normalizada: sólo sabemos que valores menores son mejores. Sin embargo, en espacios de grandes dimensionales, las distancias euclídeas tienden a aumentar (denominado como \"curse of dimensionality\")."]},{"cell_type":"code","execution_count":null,"id":"222384fe-dde4-4e6c-b62b-4c340bfb326d","metadata":{"id":"222384fe-dde4-4e6c-b62b-4c340bfb326d"},"outputs":[],"source":["kmeans.score(X)"]},{"cell_type":"markdown","id":"3c2313bb-79b2-4bdc-b156-9bb99aabb637","metadata":{"id":"3c2313bb-79b2-4bdc-b156-9bb99aabb637"},"source":["El método score() nos devuelve el valor negativo de la inercia. __¿Por qué?__"]},{"cell_type":"markdown","id":"cfe7b5b4-4a32-4a64-b6cc-f4c566b831ec","metadata":{"id":"cfe7b5b4-4a32-4a64-b6cc-f4c566b831ec"},"source":["### Obtener el valor óptimo de _K_"]},{"cell_type":"markdown","id":"22f63b11-7d18-43ae-9c57-6458635b339e","metadata":{"id":"22f63b11-7d18-43ae-9c57-6458635b339e"},"source":["Como el algoritmo k-means requiere que se determine el valor de _k_ a priori, se puede suponer que podríamos seleccionar un modelo con el menor valor de inercia. Sin embargo, esto no es tan simple"]},{"cell_type":"code","execution_count":null,"id":"bffebc05-64dd-4169-ac76-45600c0421c2","metadata":{"id":"bffebc05-64dd-4169-ac76-45600c0421c2"},"outputs":[],"source":["from sklearn.cluster import KMeans\n","kmeans_k3 = KMeans(n_clusters=3, n_init=10, random_state=1)\n","kmeans_k8 = KMeans(n_clusters=8, n_init=10, random_state=1)\n","kmeans_k3.fit(X)\n","kmeans_k8.fit(X)"]},{"cell_type":"code","execution_count":null,"id":"456f2a45-73ff-41c6-891b-2bec6693482f","metadata":{"id":"456f2a45-73ff-41c6-891b-2bec6693482f"},"outputs":[],"source":["kmeans_k3.inertia_"]},{"cell_type":"code","execution_count":null,"id":"fb8af8a7-46ce-4ead-8680-0b2189489270","metadata":{"id":"fb8af8a7-46ce-4ead-8680-0b2189489270"},"outputs":[],"source":["kmeans_k8.inertia_"]},{"cell_type":"markdown","id":"9124f881-a59f-445d-ae2e-90f47282ca00","metadata":{"id":"9124f881-a59f-445d-ae2e-90f47282ca00"},"source":["Podemos hacer una gráfica de inercia en función de _k_"]},{"cell_type":"code","execution_count":null,"id":"455e5549-98df-46dc-a51e-194f956e1107","metadata":{"id":"455e5549-98df-46dc-a51e-194f956e1107"},"outputs":[],"source":["kmeans_per_k = [ KMeans(n_clusters=k, n_init=10, random_state=1).fit(X) for k in range(1, 10) ]\n","inertias = [ model.inertia_ for model in kmeans_per_k ]\n","#kmeans_per_k es una lista de objetos kmeans entrenados"]},{"cell_type":"code","execution_count":null,"id":"b48b4a44-1881-4777-b5c4-ff8c32498c36","metadata":{"id":"b48b4a44-1881-4777-b5c4-ff8c32498c36"},"outputs":[],"source":["plt.figure(figsize=(8, 3.5))\n","plt.plot(range(1, 10), inertias, \"bo-\")\n","plt.xlabel(\"$k$\", fontsize=14)\n","plt.ylabel(\"Inercia\", fontsize=14)\n","# 'figure fraction' : (0, 0) es izq y abajo (1, 1) es derecha arriba\n","plt.annotate('Elbow',\n","             xy=(4, inertias[3]),\n","             xytext=(0.5, 0.5),\n","             textcoords='figure fraction',\n","             fontsize=16,\n","             arrowprops=dict(facecolor='black', shrink=0.1) )\n","plt.axis([1, 10, 0, 1300])\n","plt.show()"]},{"cell_type":"markdown","id":"4ff3ed52-e4f7-4613-abb0-241fc4d4a725","metadata":{"id":"4ff3ed52-e4f7-4613-abb0-241fc4d4a725"},"source":["Se puede ver que la inercia disminuye rápidamente conforme aumenta _k_ hasta 4. Después de este valor, la inercia disminuye lentamente. Teniendo en cuenta este gráfico, podríamos pensar que k=4 es una buena opción pero no podemos saber con exactitud que valor de _k_ es óptimo.\n","\n","Un enfoque más preciso (pero con más costo computacional) es el **valor de silueta** (Silhouette score o Silhouette width), el valor de silueta es una medida de cuan similar es un punto a su propio cluster en comparación con otros clústeres.\n","\n","este es el promedio de los **coeficientes de silueta** (silhouette coefficient) de cada una de las instancias.\n","\n","El valor de silueta puede tomar valores entre -1 y +1. Un valor cercano a +1 significa que la instancia se encuentra en el interior de su propio agrupamiento y alejado de otros clusters, cercano a 0 significa que la instancia está en el borde del agrupamiento y un valor cercano a -1 significa que probablemente la instancia fue mal asignada al cluster."]},{"cell_type":"code","execution_count":null,"id":"7f436e56-cc70-46fe-80ef-287bb04aaba1","metadata":{"id":"7f436e56-cc70-46fe-80ef-287bb04aaba1"},"outputs":[],"source":["from sklearn.metrics import silhouette_score"]},{"cell_type":"code","execution_count":null,"id":"da846e00-802d-4f02-ad09-7bbb75258de7","metadata":{"id":"da846e00-802d-4f02-ad09-7bbb75258de7"},"outputs":[],"source":["silhouette_scores = [silhouette_score(X, model.labels_) for model in kmeans_per_k[1:]] #por lo menos 2 clusters\n","silhouette_scores"]},{"cell_type":"markdown","id":"dfe556d3","metadata":{"id":"dfe556d3"},"source":["El **valor de silueta** es una medida que relaciona las distancias dentro de un cluster con las distancias al cluster más cercano, por esto, en el código anterior, la función silhouette_score debe recibir un modelo con por lo menos 2 agrupamientos.\n","\n","A continuación vamos a graficar el **valor de silueta** para el conjunto de datos para distintos valores de k"]},{"cell_type":"code","execution_count":null,"id":"cbb7ae93-f533-496f-a4d0-c7459aed2c78","metadata":{"id":"cbb7ae93-f533-496f-a4d0-c7459aed2c78"},"outputs":[],"source":["plt.figure(figsize=(8, 3))\n","plt.plot(range(2, 10), silhouette_scores, \"bo-\")\n","plt.xlabel(\"$k$\", fontsize=14)\n","plt.ylabel(\"Valores silueta\", fontsize=14)\n","plt.axis([1.8, 8.5, 0.55, 0.7])\n","plt.show()"]},{"cell_type":"markdown","id":"3805db25-4b2e-484d-a3d2-343a5b07f2a5","metadata":{"id":"3805db25-4b2e-484d-a3d2-343a5b07f2a5"},"source":["Ahora queda más claro que _k_= 4 o 5 son mejores opciones, lo cual no era visible comparando las inercias."]},{"cell_type":"markdown","id":"ac275144-cc89-4e69-b3d1-e9f15837d6e7","metadata":{"id":"ac275144-cc89-4e69-b3d1-e9f15837d6e7"},"source":["Una Visualización aún mas eficiente, se obtiene cuando graficamos los coeficientes de silueta de cada instancia, por orden del cluster al cual fueron asignados y por valor de coeficiente. Esto se denomina __Diagrama de Silueta__\n","\n","El diagrama de siluetas es una herramienta visual utilizada para evaluar la calidad de los clústeres generados por el algoritmo de K-Means y determinar el valor óptimo de K (número de clústeres). A diferencia del gráfico anterior donde mostrábamos valores promedio, un diagrama de siluetas muestra el coeficiente de silueta para cada punto del cluster."]},{"cell_type":"code","execution_count":null,"id":"f00874f8-df2e-4fc0-b5c4-c883a2c59f2f","metadata":{"id":"f00874f8-df2e-4fc0-b5c4-c883a2c59f2f"},"outputs":[],"source":["import matplotlib as mpl\n","from sklearn.metrics import silhouette_samples\n","from matplotlib.ticker import FixedLocator, FixedFormatter\n","\n","plt.figure(figsize=(11, 9))\n","\n","for k in (3, 4, 5, 6):\n","    plt.subplot(2, 2, k - 2)\n","\n","    y_pred = kmeans_per_k[k - 1].labels_ #etiquetas de cada instancia de X\n","    silhouette_coefficients = silhouette_samples(X, y_pred) #coeficientes para cada una de las instancias en X\n","\n","    padding = len(X) // 30\n","\n","    pos = padding\n","    ticks = []\n","    #Grafico los coeficientes de silueta por cada cluster\n","    for i in range(k): #i = 0,1,2..\n","        coeffs = silhouette_coefficients[y_pred == i] #coeficientes correspondientes a un cluster específico\n","        coeffs.sort()\n","\n","        color = mpl.cm.Spectral(i / k) #colormap\n","        plt.fill_betweenx(y = np.arange(pos, pos + len(coeffs)), x1=0, x2=coeffs, facecolor=color, edgecolor=color, alpha=0.7)\n","        ticks.append(pos + len(coeffs) // 2)\n","        pos += len(coeffs) + padding\n","\n","    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n","    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n","\n","    if k in (3, 5):\n","        plt.ylabel(\"Cluster\")\n","\n","    if k in (5, 6):\n","        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","        plt.xlabel(\"Coeficiente de silueta\")\n","    else:\n","        plt.tick_params(labelbottom=False)\n","\n","    plt.axvline(x=silhouette_scores[k - 2], color=\"red\", linestyle=\"--\")\n","    plt.title(\"$k={}$\".format(k), fontsize=16)\n","\n","plt.show()"]},{"cell_type":"markdown","id":"e699a9ee-1f09-4fbe-8408-a1799c94ded3","metadata":{"id":"e699a9ee-1f09-4fbe-8408-a1799c94ded3"},"source":["Cada diagrama tiene forma de navaja o cuchilla por cluster. La altura del diagrama indica el número de instancias que contiene el agrupamiento y su ancho representa los coeficientes de silueta de todas las instancias ordenados, más ancho es mejor porque los coeficientes de silueta se aproximan a 1. La línea de puntos indica la media de los coeficientes de silueta para el modelo."]},{"cell_type":"markdown","id":"decfeaa5-055b-49ed-924b-bd3982d4c525","metadata":{"id":"decfeaa5-055b-49ed-924b-bd3982d4c525"},"source":["Se puede ver que para k=3 y k=6 se obtienen agrupamientos \"malos\" porque tienen coeficientes de silueta bajos y se detienen antes del valor de la media (esto implica que sus instancias están demasiado cerca de otros clústeres). Para k=4 y k=5 los agrupamientos se extienden después de la línea de puntos y aunque para k=4 los coeficientes de silueta en el primer cluster son mayores, con k=5 obtenemos agrupamientos de tamaños similares."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}